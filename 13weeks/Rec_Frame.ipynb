{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMnPWB-ozGrY",
        "outputId": "2fea5d7e-988a-4913-d651-0ca18cf8eb4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/myDrive; to attempt to forcibly remount, call drive.mount(\"/content/myDrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/myDrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(use_cuda)\n",
        "if use_cuda:\n",
        "  print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k9hQQIU4Z_7",
        "outputId": "1eda7da5-bf1b-43fb-c968-c6aa3c161f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-- 3.8.16\n",
        "!wget https://www.python.org/ftp/python/3.8.16/Python-3.8.16.tgz\n",
        "!tar xvfz Python-3.8.16.tgz\n",
        "!Python-3.8.16/configure\n",
        "!sudo make install\n",
        "!python __version"
      ],
      "metadata": {
        "id": "z3l_Va2a0gK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scrapbook\n",
        "!pip install recommenders\n",
        "#-- https://github.com/microsoft/recommenders"
      ],
      "metadata": {
        "id": "0ITRG5hWzauq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install LibRecommender\n",
        "#-- https://github.com/massquantity/LibRecommender#references 라이브러리"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYDHxsVcz-5a",
        "outputId": "5475c078-52a4-4833-f6b9-e2417679665f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting LibRecommender\n",
            "  Downloading LibRecommender-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from LibRecommender) (4.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from LibRecommender) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->LibRecommender) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->LibRecommender) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->LibRecommender) (6.3.0)\n",
            "Installing collected packages: LibRecommender\n",
            "Successfully installed LibRecommender-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages.\n",
        "# !pip install torch-sparse\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "\n",
        "!pip install torch-geometric\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git\n",
        "\"\"\"\n",
        "DeepSNAP는 그래프 신경망(Graph Neural Networks)을 구축하고 훈련하기 위한 파이썬 패키지\n",
        "DeepSNAP은 PyTorch 기반의 그래프 신경망 라이브러리 PyG(PyTorch Geometric)에서 영감을 받아 개발되었다. DeepSNAP은 PyG와 유사한 인터페이스를 제공하여 그래프 데이터를 로드하고 전처리할 수 있다.\n",
        "\"\"\"\n",
        "!pip install -U -q PyDrive\n",
        "!pip install torch_scatter -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cu118.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f70IdwgkzlUb",
        "outputId": "c9664f5f-f9f4-44c7-f25c-8c348ff59dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n",
            "11.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=96b05b3ff9d7f0d5b69951d1c58e983560c4d7a073f98bda62c4e055691289cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.3.1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deepsnap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
            "Collecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_scatter\n",
            "Successfully installed torch_scatter-2.1.1+pt20cu118\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.17%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.17+pt20cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 필요 라이브러리 Import\n"
      ],
      "metadata": {
        "id": "GcP1Qvu7YKrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from libreco.data import random_split, DatasetPure\n",
        "from libreco.algorithms import LightGCN  # pure data, algorithm NGCF\n",
        "from libreco.evaluation import evaluate\n",
        "#-- Libreco\n",
        "\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn, optim, Tensor\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "from torch_geometric.utils import structured_negative_sampling\n",
        "from torch_geometric.data import download_url, extract_zip\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.typing import Adj\n",
        "#-- import required modules Blog"
      ],
      "metadata": {
        "id": "A71cke_WYMxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 준비 movielens 10만개 데이터 "
      ],
      "metadata": {
        "id": "9yKioW3OZEXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset\n",
        "url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
        "extract_zip(download_url(url, '.'), '.')\n",
        "\n",
        "rating_path = '/content/ml-latest-small/ratings.csv'\n",
        "item_path = \"/content/ml-latest-small/movies.csv\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97EgNXEjYfMt",
        "outputId": "4c304117-3859-46d8-e49a-b840a1c96460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using existing file ml-latest-small.zip\n",
            "Extracting ./ml-latest-small.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def libco_LightGCN_data(path, data_frame, ratio = [0.8, 0.1, 0.1], excluded_user=None):\n",
        "  #-- data\n",
        "  data = pd.read_csv(path)\n",
        "  data.rename(columns = {data_frame[0] : \"user\", data_frame[1]:\"item\", data_frame[2]:\"label\",},\n",
        "              inplace = True)\n",
        "  \n",
        "  # print(data.head(10))\n",
        "  #-- 테스트를 위한 특정 유저의 가장 앞에 있는 데이터 n개 삭제\n",
        "  if excluded_user is not None:\n",
        "    user_data = data[data[\"user\"] == excluded_user]\n",
        "    user_data_5stars = user_data[user_data[\"label\"] == 5.0]  # Filter the data with a rating of 5.0\n",
        "    if not user_data_5stars.empty:\n",
        "      excluded_number = 10\n",
        "      user_data_5stars = user_data_5stars.head(excluded_number)\n",
        "      data = data.drop(user_data_5stars.index)\n",
        "\n",
        "  # print(data.head(10))\n",
        "\n",
        "  # split whole data into three folds for training, evaluating and testing\n",
        "  train_data, eval_data, test_data = random_split(data, multi_ratios= ratio)\n",
        "\n",
        "  train_data, data_info = DatasetPure.build_trainset(train_data)\n",
        "  eval_data = DatasetPure.build_evalset(eval_data)\n",
        "  test_data = DatasetPure.build_testset(test_data)\n",
        "  tet_data = [train_data, eval_data, test_data]\n",
        "  print(data_info)  # n_users: 5894, n_items: 3253, data sparsity: 0.4172 %  \n",
        "  return tet_data, data_info\n",
        "  #-- Libco -- \n",
        "\n",
        "def blog_LightGCN_data(path, data_frame, rating_threshold =4, excluded_user=None):\n",
        "\n",
        "  #-- 유저, 아이템 매핑\n",
        "  rating_data = pd.read_csv(path[0], index_col= data_frame[0])\n",
        "  item_data = pd.read_csv(path[1], index_col= data_frame[1])\n",
        "  data = pd.read_csv(path[0])\n",
        "\n",
        "  print(\"1번째 데이터 삭제하기 전\")\n",
        "  print(rating_data.head(10))\n",
        "\n",
        "  #-- 테스트를 위한 특정 유저의 가장 앞에 있는 데이터 삭제\n",
        "\n",
        "  if excluded_user is not None:\n",
        "    user_data = data[data[data_frame[0]] == excluded_user]\n",
        "    user_data_5stars = user_data[user_data[data_frame[2]] == 5.0]  # Filter the data with a rating of 5.0r_user_data[r_user_data[data_frame[2]] == 5.0]  # Filter the data with a rating of 5.0\n",
        "\n",
        "    r_user_data =  rating_data[rating_data.index == excluded_user].reset_index()\n",
        "\n",
        "    if not user_data_5stars.empty:\n",
        "      excluded_number = 10\n",
        "      user_data_5stars = user_data_5stars.head(excluded_number)     \n",
        "      r_user_data_5stars = r_user_data[r_user_data[data_frame[2]] == 5.0]\n",
        "      data = data.drop(user_data_5stars.index)\n",
        "      \n",
        "      row_index = []\n",
        "      for i in range(0,excluded_number):\n",
        "        row_index.append(r_user_data_5stars.iloc[i].name)\n",
        "\n",
        "      rating_data = rating_data.reset_index().drop(row_index[0])\n",
        "      rating_data.set_index(data_frame[0], inplace=True)\n",
        "      for i in range(1, len(row_index)) :\n",
        "        rating_data = rating_data.reset_index().drop(row_index[i]-i)\n",
        "      # rating_data = rating_data.drop(rating_data.iloc.nam)\n",
        "        rating_data.set_index(data_frame[0], inplace=True)\n",
        "  \n",
        "  print(\"\\n\\n\\n1번째 데이터 삭제하기 후\")\n",
        "  print(rating_data.head(10))\n",
        "\n",
        "  # # rating_data의 컬럼 확인\n",
        "  # print(rating_data.head(5))\n",
        "\n",
        "  user_mapping = {index: i for i, index in enumerate(rating_data.index.unique())}\n",
        "  item_mapping = {index: i for i, index in enumerate(item_data.index.unique())}\n",
        "\n",
        "  #-- 매핑 데이터로 edge연결\n",
        "  edge_index = None\n",
        "  src = [user_mapping[index] for index in data[data_frame[0]]]\n",
        "  dst = [item_mapping[index] for index in data[data_frame[1]]]\n",
        "  edge_attr = torch.from_numpy(data[data_frame[2]].values).view(-1, 1).to(torch.long) >= rating_threshold\n",
        "  \n",
        "  edge_index = [[], []]\n",
        "  for i in range(edge_attr.shape[0]):\n",
        "    if edge_attr[i]:\n",
        "        edge_index[0].append(src[i])\n",
        "        edge_index[1].append(dst[i])\n",
        "  edge_index = torch.tensor(edge_index)\n",
        "\n",
        "  #-- 데이터 분할 8:1:1\n",
        "  num_users, num_movies = len(user_mapping), len(item_mapping)\n",
        "  num_interactions = edge_index.shape[1]\n",
        "  all_indices = [i for i in range(num_interactions)]\n",
        "\n",
        "  train_indices, test_indices = train_test_split(\n",
        "      all_indices, test_size=0.2, random_state=1)\n",
        "  val_indices, test_indices = train_test_split(\n",
        "      test_indices, test_size=0.5, random_state=1)\n",
        "\n",
        "  train_edge_index = edge_index[:, train_indices]\n",
        "  val_edge_index = edge_index[:, val_indices]\n",
        "  test_edge_index = edge_index[:, test_indices]\n",
        "\n",
        "  print(\"Train Edge Data : \", train_edge_index.shape)\n",
        "  print(\"Val Edge Data :\", val_edge_index.shape)\n",
        "  print(\"Test Edge Data : \", test_edge_index.shape)    \n",
        "\n",
        "  #-- 연결된 엣지를 희소행렬로 변환\n",
        "  # convert edge indices into Sparse Tensors: https://pytorch-geometric.readthedocs.io/en/latest/notes/sparse_tensor.html\n",
        "  from torch_geometric.utils import train_test_split_edges\n",
        "\n",
        "  train_sparse_edge_index = SparseTensor(row=train_edge_index[0], col=train_edge_index[1], sparse_sizes=(\n",
        "      num_users + num_movies, num_users + num_movies))\n",
        "  val_sparse_edge_index = SparseTensor(row=val_edge_index[0], col=val_edge_index[1], sparse_sizes=(\n",
        "      num_users + num_movies, num_users + num_movies))\n",
        "  test_sparse_edge_index = SparseTensor(row=test_edge_index[0], col=test_edge_index[1], sparse_sizes=(\n",
        "      num_users + num_movies, num_users + num_movies))\n",
        "\n",
        "  tet_edge_data = [edge_index, train_edge_index,val_edge_index, test_edge_index]  \n",
        "  tet_sparse_data = [train_sparse_edge_index, val_sparse_edge_index, test_sparse_edge_index]\n",
        "  return tet_edge_data, tet_sparse_data, user_mapping, item_mapping\n",
        "#-- Blog -- "
      ],
      "metadata": {
        "id": "9wh9-yIYZLsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "excluded_user_id = 1\n",
        "data_frame = [\"userId\", \"movieId\", \"rating\"]\n",
        "path = [rating_path, item_path]\n",
        "\n",
        "lib_tet_data, lib_data_info = libco_LightGCN_data(path[0], data_frame, excluded_user = excluded_user_id)\n",
        "blog_tet_edge_data, blog_tet_sparse_data, user_mapping, item_mapping = blog_LightGCN_data(path, data_frame, 4, excluded_user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtPm9dHCdvRS",
        "outputId": "ddeea212-6c72-4d48-a258-02cf0addfa77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_users: 610, n_items: 8967, data density: 1.4747 %\n",
            "1번째 데이터 삭제하기 전\n",
            "        movieId  rating  timestamp\n",
            "userId                            \n",
            "1             1     4.0  964982703\n",
            "1             3     4.0  964981247\n",
            "1             6     4.0  964982224\n",
            "1            47     5.0  964983815\n",
            "1            50     5.0  964982931\n",
            "1            70     3.0  964982400\n",
            "1           101     5.0  964980868\n",
            "1           110     4.0  964982176\n",
            "1           151     5.0  964984041\n",
            "1           157     5.0  964984100\n",
            "\n",
            "\n",
            "\n",
            "1번째 데이터 삭제하기 후\n",
            "        movieId  rating  timestamp\n",
            "userId                            \n",
            "1             1     4.0  964982703\n",
            "1             3     4.0  964981247\n",
            "1             6     4.0  964982224\n",
            "1            70     3.0  964982400\n",
            "1           110     4.0  964982176\n",
            "1           151     5.0  964984041\n",
            "1           157     5.0  964984100\n",
            "1           163     5.0  964983650\n",
            "1           216     5.0  964981208\n",
            "1           223     3.0  964980985\n",
            "Train Edge Data :  torch.Size([2, 38861])\n",
            "Val Edge Data : torch.Size([2, 4858])\n",
            "Test Edge Data :  torch.Size([2, 4858])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 정의"
      ],
      "metadata": {
        "id": "KwcANHC6t-Ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def libco_LightGCN(data_info, embed_size =32, n_epochs = 20):\n",
        "  lightgcn = LightGCN(\n",
        "      task=\"ranking\",\n",
        "      data_info= data_info,\n",
        "      loss_type=\"bpr\",\n",
        "      embed_size= embed_size,\n",
        "      n_epochs=n_epochs,\n",
        "      lr=1e-3,\n",
        "      batch_size=2048,\n",
        "      num_neg=1,\n",
        "      device=\"cuda\",\n",
        "  )\n",
        "  return lightgcn\n",
        "#-- Libco --#\n",
        "\n",
        "class Blog_LightGCN(MessagePassing):\n",
        "      def __init__(self, num_users, num_items, embedding_dim=64, K=3, add_self_loops=False):\n",
        "          super().__init__()\n",
        "          self.num_users, self.num_items = num_users, num_items\n",
        "          self.embedding_dim, self.K = embedding_dim, K\n",
        "          self.add_self_loops = add_self_loops\n",
        "          self.users_emb = nn.Embedding(\n",
        "              num_embeddings=self.num_users, embedding_dim=self.embedding_dim) # e_u^0\n",
        "          self.items_emb = nn.Embedding(\n",
        "              num_embeddings=self.num_items, embedding_dim=self.embedding_dim) # e_i^0\n",
        "          nn.init.normal_(self.users_emb.weight, std=0.1)\n",
        "          nn.init.normal_(self.items_emb.weight, std=0.1)\n",
        "      def forward(self, edge_index: SparseTensor):\n",
        "          # compute \\tilde{A}: symmetrically normalized adjacency matrix\n",
        "          edge_index_norm = gcn_norm(\n",
        "              edge_index, add_self_loops=self.add_self_loops)\n",
        "          emb_0 = torch.cat([self.users_emb.weight, self.items_emb.weight]) # E^0\n",
        "          embs = [emb_0]\n",
        "          emb_k = emb_0\n",
        "\n",
        "          # multi-scale diffusion\n",
        "          for i in range(self.K):\n",
        "              emb_k = self.propagate(edge_index_norm, x=emb_k)\n",
        "              embs.append(emb_k)\n",
        "          embs = torch.stack(embs, dim=1)\n",
        "          emb_final = torch.mean(embs, dim=1) # E^K\n",
        "          users_emb_final, items_emb_final = torch.split(\n",
        "              emb_final, [self.num_users, self.num_items]) # splits into e_u^K and e_i^K\n",
        "          # returns e_u^K, e_u^0, e_i^K, e_i^0\n",
        "          return users_emb_final, self.users_emb.weight, items_emb_final, self.items_emb.weight\n",
        "\n",
        "      def message(self, x_j: Tensor) -> Tensor:\n",
        "          return x_j\n",
        "\n",
        "      def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
        "          # computes \\tilde{A} @ x\n",
        "          return matmul(adj_t, x)\n",
        "\n",
        "#-- 손실함수 bpr\n",
        "def bpr_loss(users_emb_final, users_emb_0, pos_items_emb_final, pos_items_emb_0, neg_items_emb_final, neg_items_emb_0, lambda_val):\n",
        "    reg_loss = lambda_val * (users_emb_0.norm(2).pow(2) +\n",
        "                             pos_items_emb_0.norm(2).pow(2) +\n",
        "                             neg_items_emb_0.norm(2).pow(2)) # L2 loss\n",
        "\n",
        "    pos_scores = torch.mul(users_emb_final, pos_items_emb_final)\n",
        "    pos_scores = torch.sum(pos_scores, dim=-1) # predicted scores of positive samples\n",
        "    neg_scores = torch.mul(users_emb_final, neg_items_emb_final)\n",
        "    neg_scores = torch.sum(neg_scores, dim=-1) # predicted scores of negative samples\n",
        "\n",
        "    loss = -torch.mean(torch.nn.functional.softplus(pos_scores - neg_scores)) + reg_loss\n",
        "    return loss\n",
        "\n",
        "def blog_LightGCN(num_users, num_movies, embed_size):\n",
        "  # defines LightGCN model\n",
        "  model = Blog_LightGCN(num_users, num_movies, embedding_dim = embed_size)\n",
        "  return model\n",
        "\n",
        "#-- Blog --#"
      ],
      "metadata": {
        "id": "kNaVvFFJt9vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lib_model = libco_LightGCN(lib_data_info, embed_size =32, n_epochs = 20) \n",
        "blog_model = blog_LightGCN(len(user_mapping), len(item_mapping), embed_size= 32)"
      ],
      "metadata": {
        "id": "16GJXgzEv-KT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 학습"
      ],
      "metadata": {
        "id": "ukzkxajmzQjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def libco_fit(model, train_data, eval_data):\n",
        "  model.fit(\n",
        "        train_data,\n",
        "        neg_sampling=True,\n",
        "        verbose=1,\n",
        "        eval_data=eval_data,\n",
        "        metrics=[\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"],\n",
        "    )\n",
        "#-- Libco--#\n",
        "\n",
        "\n",
        "# function which random samples a mini-batch of positive and negative samples\n",
        "def sample_mini_batch(batch_size, edge_index):\n",
        "    edges = structured_negative_sampling(edge_index)\n",
        "    edges = torch.stack(edges, dim=0)\n",
        "    indices = random.choices(\n",
        "        [i for i in range(edges[0].shape[0])], k=batch_size)\n",
        "    batch = edges[:, indices]\n",
        "    user_indices, pos_item_indices, neg_item_indices = batch[0], batch[1], batch[2]\n",
        "    return user_indices, pos_item_indices, neg_item_indices\n",
        "\n",
        "# helper function to get N_u\n",
        "def get_user_positive_items(edge_index):\n",
        "    user_pos_items = {}\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        user = edge_index[0][i].item()\n",
        "        item = edge_index[1][i].item()\n",
        "        if user not in user_pos_items:\n",
        "            user_pos_items[user] = []\n",
        "        user_pos_items[user].append(item)\n",
        "    return user_pos_items\n",
        "\n",
        "# computes recall@K and precision@K\n",
        "def RecallPrecision_ATk(groundTruth, r, k):\n",
        "    num_correct_pred = torch.sum(r, dim=-1)  # number of correctly predicted items per user\n",
        "    # number of items liked by each user in the test set\n",
        "    user_num_liked = torch.Tensor([len(groundTruth[i])\n",
        "                                  for i in range(len(groundTruth))])\n",
        "    recall = torch.mean(num_correct_pred / user_num_liked)\n",
        "    precision = torch.mean(num_correct_pred) / k\n",
        "    return recall.item(), precision.item()\n",
        "\n",
        "# computes NDCG@K\n",
        "def NDCGatK_r(groundTruth, r, k):\n",
        "    assert len(r) == len(groundTruth)\n",
        "\n",
        "    test_matrix = torch.zeros((len(r), k))\n",
        "\n",
        "    for i, items in enumerate(groundTruth):\n",
        "        length = min(len(items), k)\n",
        "        test_matrix[i, :length] = 1\n",
        "    max_r = test_matrix\n",
        "    idcg = torch.sum(max_r * 1. / torch.log2(torch.arange(2, k + 2)), axis=1)\n",
        "    dcg = r * (1. / torch.log2(torch.arange(2, k + 2)))\n",
        "    dcg = torch.sum(dcg, axis=1)\n",
        "    idcg[idcg == 0.] = 1.\n",
        "    ndcg = dcg / idcg\n",
        "    ndcg[torch.isnan(ndcg)] = 0.\n",
        "    return torch.mean(ndcg).item()\n",
        "\n",
        "# wrapper function to get evaluation metrics\n",
        "def get_metrics(model, edge_index, exclude_edge_indices, k):\n",
        "    user_embedding = model.users_emb.weight\n",
        "    item_embedding = model.items_emb.weight\n",
        "\n",
        "    # get ratings between every user and item - shape is num users x num movies\n",
        "    rating = torch.matmul(user_embedding, item_embedding.T)\n",
        "\n",
        "    for exclude_edge_index in exclude_edge_indices:\n",
        "        # gets all the positive items for each user from the edge index\n",
        "        user_pos_items = get_user_positive_items(exclude_edge_index)\n",
        "        # get coordinates of all edges to exclude\n",
        "        exclude_users = []\n",
        "        exclude_items = []\n",
        "        for user, items in user_pos_items.items():\n",
        "            exclude_users.extend([user] * len(items))\n",
        "            exclude_items.extend(items)\n",
        "\n",
        "        # set ratings of excluded edges to large negative value\n",
        "        rating[exclude_users, exclude_items] = -(1 << 10)\n",
        "\n",
        "    # get the top k recommended items for each user\n",
        "    _, top_K_items = torch.topk(rating, k=k)\n",
        "\n",
        "    # get all unique users in evaluated split\n",
        "    users = edge_index[0].unique()\n",
        "\n",
        "    test_user_pos_items = get_user_positive_items(edge_index)\n",
        "\n",
        "    # convert test user pos items dictionary into a list\n",
        "    test_user_pos_items_list = [\n",
        "        test_user_pos_items[user.item()] for user in users]\n",
        "\n",
        "    # determine the correctness of topk predictions\n",
        "    r = []\n",
        "    for user in users:\n",
        "        ground_truth_items = test_user_pos_items[user.item()]\n",
        "        label = list(map(lambda x: x in ground_truth_items, top_K_items[user]))\n",
        "        r.append(label)\n",
        "    r = torch.Tensor(np.array(r).astype('float'))\n",
        "\n",
        "    recall, precision = RecallPrecision_ATk(test_user_pos_items_list, r, k)\n",
        "    ndcg = NDCGatK_r(test_user_pos_items_list, r, k)\n",
        "\n",
        "    return recall, precision, ndcg\n",
        "# wrapper function to evaluate model\n",
        "def evaluation(model, edge_index, sparse_edge_index, exclude_edge_indices, k, lambda_val):\n",
        "    # get embeddings\n",
        "    users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(\n",
        "        sparse_edge_index)\n",
        "    edges = structured_negative_sampling(\n",
        "        edge_index, contains_neg_self_loops=False)\n",
        "    user_indices, pos_item_indices, neg_item_indices = edges[0], edges[1], edges[2]\n",
        "    users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
        "    pos_items_emb_final, pos_items_emb_0 = items_emb_final[\n",
        "        pos_item_indices], items_emb_0[pos_item_indices]\n",
        "    neg_items_emb_final, neg_items_emb_0 = items_emb_final[\n",
        "        neg_item_indices], items_emb_0[neg_item_indices]\n",
        "\n",
        "    loss = bpr_loss(users_emb_final, users_emb_0, pos_items_emb_final, pos_items_emb_0,\n",
        "                    neg_items_emb_final, neg_items_emb_0, lambda_val).item()\n",
        "\n",
        "    recall, precision, ndcg = get_metrics(\n",
        "        model, edge_index, exclude_edge_indices, k)\n",
        "\n",
        "    return loss, recall, precision, ndcg\n",
        "def blog_fit(model, blog_tet_edge_data, blog_tet_sparse_data, epochs = 50):\n",
        "  # define contants\n",
        "  ITERATIONS = epochs\n",
        "  BATCH_SIZE = 2048\n",
        "  LR = 1e-3\n",
        "  ITERS_PER_EVAL = 300\n",
        "  ITERS_PER_LR_DECAY = 300\n",
        "  K = 10\n",
        "  LAMBDA = 1e-6\n",
        "\n",
        "  model = model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "  scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "  edge_index = blog_tet_edge_data[0].to(device)\n",
        "  train_edge_index = blog_tet_edge_data[1].to(device)\n",
        "  train_sparse_edge_index = blog_tet_sparse_data[0].to(device)\n",
        "\n",
        "  val_edge_index = blog_tet_edge_data[2].to(device)\n",
        "  val_sparse_edge_index = blog_tet_sparse_data[1].to(device)\n",
        "\n",
        "  # training loop\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  for iter in range(ITERATIONS):\n",
        "      # forward propagation\n",
        "      users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(\n",
        "          train_sparse_edge_index)\n",
        "\n",
        "      # mini batching\n",
        "      user_indices, pos_item_indices, neg_item_indices = sample_mini_batch(\n",
        "          BATCH_SIZE, train_edge_index)\n",
        "      user_indices, pos_item_indices, neg_item_indices = user_indices.to(\n",
        "          device), pos_item_indices.to(device), neg_item_indices.to(device)\n",
        "      users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
        "      pos_items_emb_final, pos_items_emb_0 = items_emb_final[\n",
        "          pos_item_indices], items_emb_0[pos_item_indices]\n",
        "      neg_items_emb_final, neg_items_emb_0 = items_emb_final[\n",
        "          neg_item_indices], items_emb_0[neg_item_indices]\n",
        "\n",
        "      # loss computation\n",
        "      train_loss = bpr_loss(users_emb_final, users_emb_0, pos_items_emb_final,\n",
        "                            pos_items_emb_0, neg_items_emb_final, neg_items_emb_0, LAMBDA)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      train_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if iter % ITERS_PER_EVAL == 0:\n",
        "          model.eval()\n",
        "          val_loss, recall, precision, ndcg = evaluation(\n",
        "              model, val_edge_index, val_sparse_edge_index, [train_edge_index], K, LAMBDA)\n",
        "          print(f\"[Iteration {iter}/{ITERATIONS}] train_loss: {round(train_loss.item(), 5)}, val_loss: {round(val_loss, 5)}, val_recall@{K}: {round(recall, 5)}, val_precision@{K}: {round(precision, 5)}, val_ndcg@{K}: {round(ndcg, 5)}\")\n",
        "          train_losses.append(train_loss.item())\n",
        "          val_losses.append(val_loss)\n",
        "          model.train()\n",
        "\n",
        "      if iter % ITERS_PER_LR_DECAY == 0 and iter != 0:\n",
        "          scheduler.step()\n",
        "  iters = [iter * ITERS_PER_EVAL for iter in range(len(train_losses))]\n",
        "  plt.plot(iters, train_losses, label='train')\n",
        "  plt.plot(iters, val_losses, label='validation')\n",
        "  plt.xlabel('iteration')\n",
        "  plt.ylabel('loss')\n",
        "  plt.title('training and validation loss curves')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "iFl61WYyubOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "libco_fit(lib_model, lib_tet_data[0], lib_tet_data[1])\n",
        "blog_fit(blog_model, blog_tet_edge_data, blog_tet_sparse_data, epochs =10000)"
      ],
      "metadata": {
        "id": "-ld8mZEpGMqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델평가"
      ],
      "metadata": {
        "id": "kmZfb3l0GRkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def libco_eval(model, test_data):\n",
        "  result = evaluate(\n",
        "      model=model,\n",
        "      data=test_data,\n",
        "      neg_sampling=True,\n",
        "      metrics=[\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"],\n",
        "  )\n",
        "  print(result)\n",
        "\n",
        "#-- Libco --\n",
        "\n",
        "def blog_eval(model, blog_tet_edge_data, blog_tet_sparse_data , K = 10):\n",
        "  # evaluate on test set\n",
        "  LAMBDA = 1e-6\n",
        "  model.eval()\n",
        "  test_edge_index = blog_tet_edge_data[3].to(device)\n",
        "  test_sparse_edge_index = blog_tet_sparse_data[2].to(device)\n",
        "\n",
        "  test_loss, test_recall, test_precision, test_ndcg = evaluation(\n",
        "              model, test_edge_index, test_sparse_edge_index, [blog_tet_edge_data[1], blog_tet_edge_data[2]], K, LAMBDA)\n",
        "\n",
        "  print(f\"[test_loss: {round(test_loss, 5)}, test_recall@{K}: {round(test_recall, 5)}, test_precision@{K}: {round(test_precision, 5)}, test_ndcg@{K}: {round(test_ndcg, 5)}\")\n"
      ],
      "metadata": {
        "id": "1NVimYSEsrB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "libco_eval(lib_model, lib_tet_data[2])\n",
        "blog_eval(blog_model, blog_tet_edge_data, blog_tet_sparse_data, 10)"
      ],
      "metadata": {
        "id": "9KRo8KwgsjPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 예측"
      ],
      "metadata": {
        "id": "-cpj2wcsGYMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lilco_predict(model, user_id, path, K = 10):\n",
        "  df = pd.read_csv(path[1])\n",
        "  rec_arr = model.recommend_user(user=user_id, n_rec=K).get(user_id)\n",
        "  \n",
        "  titles = []\n",
        "  genres = []\n",
        "  for i in rec_arr:\n",
        "    title = df[df['movieId'] == i]['title']\n",
        "    genre = df[df['movieId'] == i]['genres']\n",
        "    titles.append(title)\n",
        "    genres.append(genre)\n",
        "    # print(f\"title: {title}, genres: {genre} \")\n",
        "\n",
        "\n",
        "  return titles, genres\n",
        "#-- Libco  --#\n",
        "\n",
        "def blog_predict(model, user_id ,TopK, path, user_mapping, item_mapping, edge_index):\n",
        "\n",
        "  df = pd.read_csv(path[1])\n",
        "  movieid_title = pd.Series(df.title.values,index=df.movieId).to_dict()\n",
        "  movieid_genres = pd.Series(df.genres.values,index=df.movieId).to_dict()\n",
        "\n",
        "  user_pos_items = get_user_positive_items(edge_index)\n",
        "  user = user_mapping[user_id]\n",
        "  e_u = model.users_emb.weight[user]\n",
        "  scores = model.items_emb.weight @ e_u\n",
        "\n",
        "  values, indices = torch.topk(scores, k=len(user_pos_items[user]) + TopK)\n",
        "\n",
        "  movies = [index.cpu().item() for index in indices if index in user_pos_items[user]][:TopK]\n",
        "  movie_ids = [list(item_mapping.keys())[list(item_mapping.values()).index(movie)] for movie in movies]\n",
        "  titles = [movieid_title[id] for id in movie_ids]\n",
        "  genres = [movieid_genres[id] for id in movie_ids]\n",
        " \n",
        "  # print(f\"Here are some movies that user {user_id} rated highly\")\n",
        "  # for i in range(TopK):\n",
        "  #     print(f\"title: {titles[i]}, genres: {genres[i]} \")\n",
        "\n",
        "  movies = [index.cpu().item() for index in indices if index not in user_pos_items[user]][:TopK]\n",
        "  movie_ids = [list(item_mapping.keys())[list(item_mapping.values()).index(movie)] for movie in movies]\n",
        "  titles = [movieid_title[id] for id in movie_ids]\n",
        "  genres = [movieid_genres[id] for id in movie_ids]\n",
        "\n",
        "  # print(f\"Here are some suggested movies for user {user_id}\")\n",
        "  return titles, genres"
      ],
      "metadata": {
        "id": "utVY7FwdvZeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 1\n",
        "K = 5\n",
        "lilco_predict(lib_model,user_id, path, K = K)\n",
        "blog_predict(blog_model, user_id , K, path, user_mapping, item_mapping, blog_tet_edge_data[0])"
      ],
      "metadata": {
        "id": "ZQaelhvfHLsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 프레임워크"
      ],
      "metadata": {
        "id": "YCViZvEuu8hS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def libco(data_frame, path, embed_size, n_epochs, excluded_user):\n",
        "  lib_tet_data, lib_data_info = libco_LightGCN_data(path[0], data_frame, excluded_user = excluded_user_id)\n",
        "  lib_model = libco_LightGCN(lib_data_info, embed_size =embed_size, n_epochs = int(n_epochs/3)) \n",
        "  libco_fit(lib_model, lib_tet_data[0], lib_tet_data[1])\n",
        "  libco_eval(lib_model, lib_tet_data[2])\n",
        "  titles, genres = lilco_predict(lib_model,user_id, path, K = K)\n",
        "  return titles,genres\n",
        "  \n",
        "def blog(data_frame, path, embed_size, n_epochs, excluded_user):\n",
        "  blog_tet_edge_data, blog_tet_sparse_data, user_mapping, item_mapping = blog_LightGCN_data(path, data_frame, 4, excluded_user_id)\n",
        "  blog_model = blog_LightGCN(len(user_mapping), len(item_mapping), embed_size= 32)\n",
        "  blog_fit(blog_model, blog_tet_edge_data, blog_tet_sparse_data, epochs =n_epochs)\n",
        "  blog_eval(blog_model, blog_tet_edge_data, blog_tet_sparse_data, 10)\n",
        "  titles, genres = blog_predict(blog_model, user_id , K, path, user_mapping, item_mapping, blog_tet_edge_data[0])\n",
        "  return titles,genres"
      ],
      "metadata": {
        "id": "xBTAeMaAGbPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_frame = [\"userId\", \"movieId\", \"rating\"]\n",
        "path = [rating_path, item_path]\n",
        "embed_size = 16\n",
        "n_epochs = 20000\n",
        "user_id = 1\n",
        "excluded_user = 1\n",
        "K = 15\n",
        "\n",
        "\n",
        "lib_titles, lib_genres = libco(data_frame, path,embed_size, n_epochs, excluded_user)\n",
        "blog_titles, blog_genres = blog(data_frame, path, embed_size, n_epochs, excluded_user)"
      ],
      "metadata": {
        "id": "cPSo9MMM68Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(K):\n",
        "      print(f\"title: {lib_titles[i].values}, genres: {lib_genres[i].values} \")\n",
        "print(\"========================================================\")\n",
        "for i in range(K):\n",
        "      print(f\"title: {blog_titles[i]}, genres: {blog_genres[i]} \")      \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxLG09QLEfuJ",
        "outputId": "c7e8f6ad-6238-4ef6-8e8f-c8741af8b429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "title: ['Absent-Minded Professor, The (1961)'], genres: ['Children|Comedy|Fantasy'] \n",
            "title: ['NeverEnding Story, The (1984)'], genres: ['Adventure|Children|Fantasy'] \n",
            "title: ['NeverEnding Story III, The (1994)'], genres: ['Adventure|Children|Fantasy'] \n",
            "title: ['Friday the 13th (1980)'], genres: ['Horror|Mystery|Thriller'] \n",
            "title: ['Indiana Jones and the Temple of Doom (1984)'], genres: ['Action|Adventure|Fantasy'] \n",
            "title: ['Fantasia (1940)'], genres: ['Animation|Children|Fantasy|Musical'] \n",
            "title: ['Aliens (1986)'], genres: ['Action|Adventure|Horror|Sci-Fi'] \n",
            "title: ['Balto (1995)'], genres: ['Adventure|Animation|Children'] \n",
            "title: ['Antz (1998)'], genres: ['Adventure|Animation|Children|Comedy|Fantasy'] \n",
            "title: ['Jaws (1975)'], genres: ['Action|Horror'] \n",
            "title: ['Stargate (1994)'], genres: ['Action|Adventure|Sci-Fi'] \n",
            "title: ['Star Wars: Episode I - The Phantom Menace (1999)'], genres: ['Action|Adventure|Sci-Fi'] \n",
            "title: ['Honey, I Shrunk the Kids (1989)'], genres: ['Adventure|Children|Comedy|Fantasy|Sci-Fi'] \n",
            "title: ['Batman (1989)'], genres: ['Action|Crime|Thriller'] \n",
            "title: ['Star Trek: The Motion Picture (1979)'], genres: ['Adventure|Sci-Fi'] \n",
            "========================================================\n",
            "title: Shawshank Redemption, The (1994), genres: Crime|Drama \n",
            "title: Pulp Fiction (1994), genres: Comedy|Crime|Drama|Thriller \n",
            "title: Star Wars: Episode IV - A New Hope (1977), genres: Action|Adventure|Sci-Fi \n",
            "title: Usual Suspects, The (1995), genres: Crime|Mystery|Thriller \n",
            "title: Terminator 2: Judgment Day (1991), genres: Action|Sci-Fi \n",
            "title: Godfather, The (1972), genres: Crime|Drama \n",
            "title: Seven (a.k.a. Se7en) (1995), genres: Mystery|Thriller \n",
            "title: Lord of the Rings: The Return of the King, The (2003), genres: Action|Adventure|Drama|Fantasy \n",
            "title: Lord of the Rings: The Two Towers, The (2002), genres: Adventure|Fantasy \n",
            "title: Lord of the Rings: The Fellowship of the Ring, The (2001), genres: Adventure|Fantasy \n",
            "title: Apollo 13 (1995), genres: Adventure|Drama|IMAX \n",
            "title: Memento (2000), genres: Mystery|Thriller \n",
            "title: One Flew Over the Cuckoo's Nest (1975), genres: Drama \n",
            "title: Sixth Sense, The (1999), genres: Drama|Horror|Mystery \n",
            "title: Shrek (2001), genres: Adventure|Animation|Children|Comedy|Fantasy|Romance \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(path[0])\n",
        "user_data = data[data[data_frame[0]] == excluded_user]\n",
        "user_data_5stars = user_data[user_data[data_frame[2]] == 5.0]  # Filter the data with a rating of 5.0r_user_data[r_user_data[data_frame[2]] == 5.0]  # Filter the data with a rating of 5.0\n",
        "# print(user_data_5stars.head(10))\n",
        "data = user_data_5stars.head(15)\n",
        "print(data['movieId'].values)\n",
        "moviedIds = data['movieId'].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTR4I1gFDksZ",
        "outputId": "1ce08e6b-f437-4d19-f704-5c6ca5c21c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 47  50 101 151 157 163 216 231 260 333 362 457 527 553 596]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path[1])\n",
        "\n",
        "titles = []\n",
        "genres = []\n",
        "\n",
        "for i in moviedIds:\n",
        "    title = df[df['movieId'] == i]['title']\n",
        "    genre = df[df['movieId'] == i]['genres']\n",
        "    titles.append(title)\n",
        "    genres.append(genre)\n",
        "for i in range(len(moviedIds)):\n",
        "  print(f\"title: {titles[i].values}, genres: {genres[i].values} \")\n",
        "\n",
        "\"\"\"\n",
        "title: ['Seven (a.k.a. Se7en) (1995)'], genres: ['Mystery|Thriller']  ->\n",
        "title: ['Usual Suspects, The (1995)'], genres: ['Crime|Mystery|Thriller']  ->\n",
        "title: ['Bottle Rocket (1996)'], genres: ['Adventure|Comedy|Crime|Romance'] \n",
        "title: ['Rob Roy (1995)'], genres: ['Action|Drama|Romance|War'] \n",
        "title: ['Canadian Bacon (1995)'], genres: ['Comedy|War'] \n",
        "title: ['Desperado (1995)'], genres: ['Action|Romance|Western'] \n",
        "title: ['Billy Madison (1995)'], genres: ['Comedy'] \n",
        "title: ['Dumb & Dumber (Dumb and Dumber) (1994)'], genres: ['Adventure|Comedy'] \n",
        "title: ['Star Wars: Episode IV - A New Hope (1977)'], genres: ['Action|Adventure|Sci-Fi'] ->\n",
        "title: ['Tommy Boy (1995)'], genres: ['Comedy'] \n",
        "title: ['Jungle Book, The (1994)'], genres: ['Adventure|Children|Romance'] \n",
        "title: ['Fugitive, The (1993)'], genres: ['Thriller'] \n",
        "title: [\"Schindler's List (1993)\"], genres: ['Drama|War'] \n",
        "title: ['Tombstone (1993)'], genres: ['Action|Drama|Western'] \n",
        "title: ['Pinocchio (1940)'], genres: ['Animation|Children|Fantasy|Musical'] \n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "FUNhHGSisqqv",
        "outputId": "7974b924-f400-4905-de85-e9f8cbb32228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "title: ['Seven (a.k.a. Se7en) (1995)'], genres: ['Mystery|Thriller'] \n",
            "title: ['Usual Suspects, The (1995)'], genres: ['Crime|Mystery|Thriller'] \n",
            "title: ['Bottle Rocket (1996)'], genres: ['Adventure|Comedy|Crime|Romance'] \n",
            "title: ['Rob Roy (1995)'], genres: ['Action|Drama|Romance|War'] \n",
            "title: ['Canadian Bacon (1995)'], genres: ['Comedy|War'] \n",
            "title: ['Desperado (1995)'], genres: ['Action|Romance|Western'] \n",
            "title: ['Billy Madison (1995)'], genres: ['Comedy'] \n",
            "title: ['Dumb & Dumber (Dumb and Dumber) (1994)'], genres: ['Adventure|Comedy'] \n",
            "title: ['Star Wars: Episode IV - A New Hope (1977)'], genres: ['Action|Adventure|Sci-Fi'] \n",
            "title: ['Tommy Boy (1995)'], genres: ['Comedy'] \n",
            "title: ['Jungle Book, The (1994)'], genres: ['Adventure|Children|Romance'] \n",
            "title: ['Fugitive, The (1993)'], genres: ['Thriller'] \n",
            "title: [\"Schindler's List (1993)\"], genres: ['Drama|War'] \n",
            "title: ['Tombstone (1993)'], genres: ['Action|Drama|Western'] \n",
            "title: ['Pinocchio (1940)'], genres: ['Animation|Children|Fantasy|Musical'] \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntitle: [\\'Seven (a.k.a. Se7en) (1995)\\'], genres: [\\'Mystery|Thriller\\']  ->\\ntitle: [\\'Usual Suspects, The (1995)\\'], genres: [\\'Crime|Mystery|Thriller\\']  ->\\ntitle: [\\'Bottle Rocket (1996)\\'], genres: [\\'Adventure|Comedy|Crime|Romance\\'] \\ntitle: [\\'Rob Roy (1995)\\'], genres: [\\'Action|Drama|Romance|War\\'] \\ntitle: [\\'Canadian Bacon (1995)\\'], genres: [\\'Comedy|War\\'] \\ntitle: [\\'Desperado (1995)\\'], genres: [\\'Action|Romance|Western\\'] \\ntitle: [\\'Billy Madison (1995)\\'], genres: [\\'Comedy\\'] \\ntitle: [\\'Dumb & Dumber (Dumb and Dumber) (1994)\\'], genres: [\\'Adventure|Comedy\\'] \\ntitle: [\\'Star Wars: Episode IV - A New Hope (1977)\\'], genres: [\\'Action|Adventure|Sci-Fi\\'] ->\\ntitle: [\\'Tommy Boy (1995)\\'], genres: [\\'Comedy\\'] \\ntitle: [\\'Jungle Book, The (1994)\\'], genres: [\\'Adventure|Children|Romance\\'] \\ntitle: [\\'Fugitive, The (1993)\\'], genres: [\\'Thriller\\'] \\ntitle: [\"Schindler\\'s List (1993)\"], genres: [\\'Drama|War\\'] \\ntitle: [\\'Tombstone (1993)\\'], genres: [\\'Action|Drama|Western\\'] \\ntitle: [\\'Pinocchio (1940)\\'], genres: [\\'Animation|Children|Fantasy|Musical\\'] \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(K):\n",
        "  if(lib_titles[i].values  == titles[i].values):\n",
        "      print(f\"title: {lib_titles[i].values}, genres: {lib_genres[i].values} \")"
      ],
      "metadata": {
        "id": "2RYu-hbO2_ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "신규 데이터를 어떻게 예측\n",
        "- 이미 평가를 한 유저\n",
        "- 콜드 스타트 유저\n",
        "데이터 전처리 \n",
        "추천에서 정확도를 어떻게 평가하는지 기준이 필요\n",
        "- 데이터에 따라 틀림"
      ],
      "metadata": {
        "id": "r0KlSGwK60yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "유사도 모델기반"
      ],
      "metadata": {
        "id": "z3vRhC6W5R89"
      }
    }
  ]
}